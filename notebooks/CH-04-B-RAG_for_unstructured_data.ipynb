{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 04-B - Generación Aumentada con Recuperación (RAG) para Datos No Estructurados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introducción\n",
    "\n",
    "Las empresas tienen mucha información propietaria que debe tenerse en cuenta al responder las preguntas de los usuarios; estas no siempre pueden ser respondidas a través de los datos con los que se han entrenado los modelos GPT.\n",
    "\n",
    "En el último notebook, trabajamos principalmente con datos estructurados. Muchas veces, los datos de tu empresa no se limitan solo a formatos estructurados como archivos CSV o tablas SQL. También pueden incluir datos no estructurados como documentos PDF o imágenes. De hecho, tus documentos individuales podrían tener tanto datos no estructurados como estructurados integrados. Extraer información de estos formatos diversos de una manera comprensible presenta un desafío. Herramientas como Azure Document Intelligence permiten la extracción de datos de fuentes no estructuradas como formularios o documentos. Una vez que los datos se extraen en un formato JSON estructurado, se puede utilizar AI Search para consolidar toda la información de diferentes tipos de datos en índices, facilitando la recuperación de documentos relevantes.\n",
    "\n",
    "En este notebook, te guiaremos a través de un caso de uso de Generación Aumentada con Recuperación (RAG) que implica trabajar con datos no estructurados. El enfoque RAG combina varias tecnologías para mejorar la calidad y la relevancia de las salidas generadas. Aprovecharemos Azure Document Intelligence para procesar documentos complejos, utilizando la API de layout para extraer texto y tablas de manera efectiva. Utilizaremos Azure AI Search para crear un índice configurando capacidades de búsqueda semántica, lo que permite la recuperación de páginas de documentos relevantes. Además, se incorporarán embeddings para recuperar contenido que esté lo más alineado posible con la pregunta del usuario. Finalmente, el modelo ChatGPT de Azure OpenAI utilizará el contenido extraído para generar una respuesta más significativa. Es importante enfatizar que este proceso de grounding (fundamentación) sigue el patrón RAG mencionado en el cuaderno anterior y ayuda a eliminar inexactitudes en las respuestas generadas.\n",
    "\n",
    "Tus objetivos para este desafío son leer este notebook, ejecutar cada bloque de código, observar los resultados y luego poder responder las preguntas planteadas en la guía del estudiante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken==0.5.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tiktoken==0.5.2) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from tiktoken==0.5.2) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.2) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "! pip install \"tiktoken==0.5.2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Azure Forms Recognizer, Azure Cognitive Search, OpenAI, and other python modules\n",
    "\n",
    "import os, json, requests, sys, re\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSettings\n",
    ")\n",
    "\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "import openai\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is secure and recommended way to load OpenAI resource credentials and deployment names\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "chat_model = os.environ['CHAT_MODEL_NAME']\n",
    "embedding_model=os.environ['EMBEDDING_MODEL_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA:** La ruta en la celda de código a continuación se refiere a la carpeta `/data/unstructured/raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- raw data\n",
    "RAW_DATA_FOLDER= '../data/unstructured/raw'\n",
    "# -- extracted json file \n",
    "EXTRACTED_DATA_FOLDER = '../data/unstructured/extracted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "endpoint = os.environ[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n",
    "key = os.environ[\"AZURE_FORM_RECOGNIZER_KEY\"]\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos extraer los datos de nuestros datos no estructurados a un formato más legible para que el modelo los entienda. La herramienta Document Intelligence nos ayuda a hacerlo aprovechando los modelos de diseño preconstruidos. Aquí, trabajamos principalmente con archivos PDF, pero también podríamos tener formatos JPG y PNG que la herramienta Document Intelligence también admite.\n",
    "\n",
    "Para cada documento, queremos especificar la forma en que se extrae la información. Por ejemplo, en este caso de uso, cada documento tiene muchas páginas. Para hacer un seguimiento de las páginas, las almacenamos en el campo page_number. También queremos extraer el contenido de cada página y colocarlo en un campo page_context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_local_single_file(file_name: str):\n",
    "    not_completed = True\n",
    "    while not_completed:\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            poller = document_analysis_client.begin_analyze_document(\n",
    "                \"prebuilt-layout\", document=f\n",
    "            )\n",
    "            not_completed=False\n",
    "    result = poller.result()\n",
    "    return get_page_content(file_name, result)\n",
    "\n",
    "def extract_files( folder_name: str, destination_folder_name: str):\n",
    "    os.makedirs(destination_folder_name, exist_ok=True)\n",
    "    for file in os.listdir(folder_name):\n",
    "        if file[-3:].upper() in ['PDF','JPG','PNG']:\n",
    "            print('Processing file:', file, end='')\n",
    "        \n",
    "            page_content = extract_local_single_file(os.path.join(folder_name, file))\n",
    "            output_file = os.path.join(destination_folder_name, file[:-3] +'json')\n",
    "            print(f'  write output to {output_file}')\n",
    "            with open(output_file, \"w\") as f:\n",
    "                f.write(json.dumps(page_content))\n",
    "\n",
    "\n",
    "def get_page_content(file_name:str, result):\n",
    "    page_content = []\n",
    "    for page in result.pages:\n",
    "        all_lines_content = []\n",
    "        for line_idx, line in enumerate(page.lines):\n",
    "            all_lines_content.append(' '.join([word.content for word in line.get_words()]))\n",
    "        page_content.append({'page_number':page.page_number, \n",
    "                                'page_content':' '.join(all_lines_content)})\n",
    "    return {'filename':file_name, 'content':page_content}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  write output to ../data/unstructured/extracted/Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.json\n",
      "Processing file: Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.pdf  write output to ../data/unstructured/extracted/Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.json\n",
      "Processing file: Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf  write output to ../data/unstructured/extracted/Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.json\n"
     ]
    }
   ],
   "source": [
    "extract_files(RAW_DATA_FOLDER, EXTRACTED_DATA_FOLDER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Más Sobre Nuestros Datos\n",
    "\n",
    "En este tutorial, examinaremos varios artículos de investigación sobre temas de LLM en documentos PDF. Esto incluye temas como:\n",
    "\n",
    "- Autoprompting (prompting automático)\n",
    "- Chain of thought prompting (prompting de cadena de pensamiento)\n",
    "- precise zero shot dense retrival (recuperación densa precisa de cero disparos)\n",
    "- y más. \n",
    "\n",
    "Este conjunto de datos contiene varios formatos no estructurados, como texto, tablas, gráficos y fórmulas.\n",
    "\n",
    "## Descripción de los Datos\n",
    "\n",
    "El esquema relevante para nuestro trabajo de hoy consiste en:\n",
    "\n",
    "- document_id\n",
    "- document_name\n",
    "- file_path\n",
    "- page_number\n",
    "- page_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "for file in os.listdir(EXTRACTED_DATA_FOLDER):\n",
    "    with open(os.path.join(EXTRACTED_DATA_FOLDER, file)) as f:\n",
    "        page_content= json.loads(f.read())\n",
    "    documents.extend(\n",
    "        [\n",
    "            {\n",
    "                'document_id':page_content['filename'].split('\\\\')[-1].split('.')[0] + '-' + str(page['page_number']),\n",
    "                'document_name':page_content['filename'].split('\\\\')[-1],\n",
    "                'file_path':page_content['filename'],              \n",
    "                'page_number':page['page_number'],\n",
    "                'page_text':page['page_content']\n",
    "            }\n",
    "            for page in page_content['content']\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': '-2',\n",
       " 'document_name': '../data/unstructured/raw/Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf',\n",
       " 'file_path': '../data/unstructured/raw/Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf',\n",
       " 'page_number': 2,\n",
       " 'page_text': 'Model Tuning Pre-trained Model (11B params) Prompt Tuning a1 a2 Task A Batch b1 Task B Batch c1 Task C c2 Batch Mixed-task Batch Task A Model (11B params) A a1 A C c1 b1 a2 B A B Task B Model (11B params) -- C c2 C Task Prompts (20K params each) Task C Model (11B params) ---- Pre-trained Model (11B params) Figure 2: Model tuning requires making a task- specific copy of the entire pre-trained model for each downstream task and inference must be performed in separate batches. Prompt tuning only requires stor- ing a small task-specific prompt for each task, and enables mixed-task inference using the original pre- trained model. With a T5 “XXL” model, each copy of the tuned model requires 11 billion parameters. By contrast, our tuned prompts would only require 20,480 parameters per task—a reduction of over five orders of magnitude—assuming a prompt length of 5 tokens. low fine-tuned T5-XXL (Raffel et al., 2020) (71.8 vs. 89.3) despite using 16 times more parameters. Several efforts to automate prompt design have been recently proposed. Shin et al. (2020) propose a search algorithm over the discrete space of words, guided by the downstream application training data. While this technique outperforms manual prompt design, there is still a gap relative to model tuning. Li and Liang (2021) propose “prefix tuning” and show strong results on generative tasks. This method freezes the model parameters and back- propagates the error during tuning to prefix ac- tivations prepended to each layer in the encoder stack, including the input layer. Hambardzumyan et al. (2021) simplify this recipe by restricting the trainable parameters to the input and output sub- networks of a masked language model, and show reasonable results on classifications tasks. In this paper, we propose prompt tuning as a further simplification for adapting language models. We freeze the entire pre-trained model and only al- low an additional k tunable tokens per downstream task to be prepended to the input text. This “soft prompt” is trained end-to-end and can condense the signal from a full labeled dataset, allowing our method to outperform few-shot prompts and close the quality gap with model tuning (Figure 1). At the same time, since a single pre-trained model is recycled for all downstream tasks, we retain the ef- ficient serving benefits of frozen models (Figure 2). While we developed our method concurrently with Li and Liang (2021) and Hambardzumyan et al. (2021), we are the first to show that prompt tuning alone (with no intermediate-layer prefixes or task-specific output layers) is sufficient to be com- petitive with model tuning. Through detailed ex- periments in sections 2–3, we demonstrate that lan- guage model capacity is a key ingredient for these approaches to succeed. As Figure 1 shows, prompt tuning becomes more competitive with scale. We compare with similar approaches in Sec- tion 4. Explicitly separating task-specific param- eters from the “generalist” parameters needed for general language-understanding has a range of ad- ditional benefits. We show in Section 5 that by capturing the task definition in the prompt while keeping the generalist parameters fixed, we are able to achieve better resilience to domain shifts. In Sec- tion 6, we show that “prompt ensembling”, learn- ing multiple prompts for the same task, can boost quality and is more efficient than classic model en- sembling. Finally, in Section 7, we investigate the interpretability of our learned soft prompts. In sum, our key contributions are: 1. Proposing prompt tuning and showing its com- petitiveness with model tuning in the regime of large language models. 2. Ablating many design choices, and showing quality and robustness improve with scale. 3. Showing prompt tuning outperforms model tuning on domain shift problems. 4. Proposing “prompt ensembling” and showing its effectiveness. 2 Prompt Tuning Following the “text-to-text” approach of T5 (Raffel et al., 2020), we cast all tasks as text generation. Instead of modeling classification as the probabil- ity of an output class given some input, Pr(y|X), where X is a series of tokens and y is a single class label, we now model it as conditional generation, where Y is a sequence of tokens that represent a class label. T5 models classification as Prθ(Y |X), parameterized by the weights, θ, of the transform- ers (Vaswani et al., 2017) that make up its encoder and decoder. Prompting is the approach of adding extra in- formation for the model to condition on during its generation of Y . Normally, prompting is done by prepending a series of tokens, P, to the in- put X, such that the model maximizes the likeli- hood of the correct Y , Prθ(Y |[P; X]), while keep-'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of a single page of research paper file that will be indexed in Azure Cognitive Search\n",
    "documents[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta sección se centrará en AI Search y los siguientes temas:\n",
    "\n",
    "1. Crear un índice de cliente\n",
    "2. Definir los campos del índice con los atributos necesarios\n",
    "3. Crear una configuración semántica\n",
    "4. Cargar nuestro índice con las páginas de los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.search.documents.indexes._search_index_client.SearchIndexClient at 0x7392c3f459a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an SDK client\n",
    "service_endpoint = os.getenv(\"AZURE_COGNITIVE_SEARCH_ENDPOINT\")   \n",
    "key = os.getenv(\"AZURE_COGNITIVE_SEARCH_KEY\")\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "index_name = os.getenv(\"AZURE_COGNITIVE_SEARCH_DOC_INDEX_NAME\")\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "index_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " research-paper-index-labp created\n"
     ]
    }
   ],
   "source": [
    "fields = [\n",
    "    SimpleField(name=\"document_id\", type=SearchFieldDataType.String, key=True),\n",
    "    SimpleField(name=\"page_number\", type=SearchFieldDataType.Int64),\n",
    "    SimpleField(name=\"file_path\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"document_name\", type=SearchFieldDataType.String,\n",
    "                searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"page_text\", type=SearchFieldDataType.String,\n",
    "                filterable=True, searchable=True, retrievable=True),\n",
    "]\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"document_id\"),\n",
    "        prioritized_keywords_fields=[SemanticField(field_name=\"document_name\")],\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"page_text\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 41 documents\n"
     ]
    }
   ],
   "source": [
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f\"Uploaded {len(documents)} documents\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Aquí vemos a Azure AI Search en acción! Podemos recuperar los documentos más relevantes de todos con los que estamos trabajando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which is promp tuning?\"\n",
    "count = 10\n",
    "results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "page_chunks = []\n",
    "citations = []\n",
    "for result in results:\n",
    "    page_chunks.append(result['page_text'])\n",
    "    citations.append(result['document_name'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and length normalization of 0.8. Decoding take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0 20 30 Prefix Length (XSUM) 21.0- 36.0 20.5 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scription of a data table, as shown in Figure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prefix-Tuning: Optimizing Continuous Prompts f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>significantly improves generation, as shown in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Source [Unseen, Athelete] (Al Kharaitiyat SC, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15 36 rouge-1 14 28 0 13 rouge-2 2 34 32 100 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Autoregressive Model (e.g. GPT2) Summarization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Self-Consistency Improves Chain of Thought Rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BLEU NIST MET E2E R-L CIDEr BLEU S U A WebNLG ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks\n",
       "0  and length normalization of 0.8. Decoding take...\n",
       "1  0 20 30 Prefix Length (XSUM) 21.0- 36.0 20.5 3...\n",
       "2  scription of a data table, as shown in Figure ...\n",
       "3  Prefix-Tuning: Optimizing Continuous Prompts f...\n",
       "4  significantly improves generation, as shown in...\n",
       "5  Source [Unseen, Athelete] (Al Kharaitiyat SC, ...\n",
       "6  15 36 rouge-1 14 28 0 13 rouge-2 2 34 32 100 2...\n",
       "7  Autoregressive Model (e.g. GPT2) Summarization...\n",
       "8  Self-Consistency Improves Chain of Thought Rea...\n",
       "9  BLEU NIST MET E2E R-L CIDEr BLEU S U A WebNLG ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df = pd.DataFrame(page_chunks, columns = [\"page_chunks\"]) #datframe with document chunks\n",
    "embed_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tengamos los documentos más relevantes, crearemos embeddings para todos los fragmentos de página. Esto nos ayudará a encontrar los documentos más similares a nuestra consulta de usuario dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Rate Limits\n",
    "\n",
    "from openai.error import RateLimitError\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def get_embedding(text: str, engine: str = \"text-embedding-ada-002\"):\n",
    "    count=0\n",
    "    while True:\n",
    "        try:\n",
    "            embedding = openai.Embedding().create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n",
    "            break;\n",
    "        except RateLimitError:\n",
    "            count+=1\n",
    "            #print(f'RateLimitError Count: {count}')\n",
    "            sleep(2)            \n",
    "    return np.array(embedding).astype(np.float32)\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-35-turbo\"): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "embed_df['embedding'] = embed_df[\"page_chunks\"].apply(lambda page_text : get_embedding(page_text, engine = embedding_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and length normalization of 0.8. Decoding take...</td>\n",
       "      <td>[0.0021886565, 0.010024328, 0.01578358, -0.023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0 20 30 Prefix Length (XSUM) 21.0- 36.0 20.5 3...</td>\n",
       "      <td>[-0.0014705374, 0.0114136385, 0.022483494, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scription of a data table, as shown in Figure ...</td>\n",
       "      <td>[-0.017270068, 0.006634906, 0.005179641, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prefix-Tuning: Optimizing Continuous Prompts f...</td>\n",
       "      <td>[-0.018153809, 0.0046602665, 0.0045732562, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>significantly improves generation, as shown in...</td>\n",
       "      <td>[-0.0019187041, 0.008303899, 0.009666913, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Source [Unseen, Athelete] (Al Kharaitiyat SC, ...</td>\n",
       "      <td>[0.006919452, -0.0010013161, 0.010834084, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15 36 rouge-1 14 28 0 13 rouge-2 2 34 32 100 2...</td>\n",
       "      <td>[0.0015339841, -0.00731552, 0.016493173, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Autoregressive Model (e.g. GPT2) Summarization...</td>\n",
       "      <td>[-0.018188149, 0.015794247, 0.015863037, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Self-Consistency Improves Chain of Thought Rea...</td>\n",
       "      <td>[0.0022468215, 0.018690253, 0.009998872, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BLEU NIST MET E2E R-L CIDEr BLEU S U A WebNLG ...</td>\n",
       "      <td>[0.006841395, 0.0026137456, 0.011311484, -0.02...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks  \\\n",
       "0  and length normalization of 0.8. Decoding take...   \n",
       "1  0 20 30 Prefix Length (XSUM) 21.0- 36.0 20.5 3...   \n",
       "2  scription of a data table, as shown in Figure ...   \n",
       "3  Prefix-Tuning: Optimizing Continuous Prompts f...   \n",
       "4  significantly improves generation, as shown in...   \n",
       "5  Source [Unseen, Athelete] (Al Kharaitiyat SC, ...   \n",
       "6  15 36 rouge-1 14 28 0 13 rouge-2 2 34 32 100 2...   \n",
       "7  Autoregressive Model (e.g. GPT2) Summarization...   \n",
       "8  Self-Consistency Improves Chain of Thought Rea...   \n",
       "9  BLEU NIST MET E2E R-L CIDEr BLEU S U A WebNLG ...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.0021886565, 0.010024328, 0.01578358, -0.023...  \n",
       "1  [-0.0014705374, 0.0114136385, 0.022483494, -0....  \n",
       "2  [-0.017270068, 0.006634906, 0.005179641, -0.00...  \n",
       "3  [-0.018153809, 0.0046602665, 0.0045732562, -0....  \n",
       "4  [-0.0019187041, 0.008303899, 0.009666913, -0.0...  \n",
       "5  [0.006919452, -0.0010013161, 0.010834084, -0.0...  \n",
       "6  [0.0015339841, -0.00731552, 0.016493173, -0.00...  \n",
       "7  [-0.018188149, 0.015794247, 0.015863037, -0.01...  \n",
       "8  [0.0022468215, 0.018690253, 0.009998872, -0.02...  \n",
       "9  [0.006841395, 0.0026137456, 0.011311484, -0.02...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prefix-Tuning: Optimizing Continuous Prompts f...</td>\n",
       "      <td>[-0.018153809, 0.0046602665, 0.0045732562, -0....</td>\n",
       "      <td>0.815534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scription of a data table, as shown in Figure ...</td>\n",
       "      <td>[-0.017270068, 0.006634906, 0.005179641, -0.00...</td>\n",
       "      <td>0.804703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>significantly improves generation, as shown in...</td>\n",
       "      <td>[-0.0019187041, 0.008303899, 0.009666913, -0.0...</td>\n",
       "      <td>0.779101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks  \\\n",
       "0  Prefix-Tuning: Optimizing Continuous Prompts f...   \n",
       "1  scription of a data table, as shown in Figure ...   \n",
       "2  significantly improves generation, as shown in...   \n",
       "\n",
       "                                           embedding  similarities  \n",
       "0  [-0.018153809, 0.0046602665, 0.0045732562, -0....      0.815534  \n",
       "1  [-0.017270068, 0.006634906, 0.005179641, -0.00...      0.804703  \n",
       "2  [-0.0019187041, 0.008303899, 0.009666913, -0.0...      0.779101  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding = get_embedding(query, engine=embedding_model)\n",
    "embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "top_results = (\n",
    "    embed_df.sort_values(\"similarities\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    "    .head(3)\n",
    ")\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
      "Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
      "\n",
      "User Query: ```Which is promp tuning?```\n",
      "List of Extracted Pages: ```['Prefix-Tuning: Optimizing Continuous Prompts for Generation arXiv:2101.00190v1 [cs.CL] 1 Jan 2021 Xiang Lisa Li Stanford University xlisali@stanford.edu Abstract Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for nat- ural language generation tasks, which keeps language model parameters frozen, but opti- mizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspira- tion from prompting, allowing subsequent to- kens to attend to this prefix as if it were “vir- tual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data set- ting, outperforms fine-tuning in low-data set- tings, and extrapolates better to examples with topics unseen during training. 1 Introduction Fine-tuning is the prevalent paradigm for using large pretrained language models (LMs) (Radford et al., 2019; Devlin et al., 2019) to perform down- stream tasks (e.g., summarization), but it requires updating and storing all the parameters of the LM. Consequently, to build and deploy NLP systems that rely on large pretrained LMs, one currently needs to store a modified copy of the LM parame- ters for each task. This can be prohibitively expen- sive, given the large size of current LMs; for exam- ple, GPT-2 has 774M parameters (Radford et al., 2019) and GPT-3 has 175B parameters (Brown et al., 2020). A natural approach to this problem is lightweight fine-tuning, which freezes most of the pretrained parameters and augments the model with small trainable modules. For example, adapter-tuning Percy Liang Stanford University pliang@cs.stanford.edu Figure 1: Fine-tuning (top) updates all Transformer parameters (the red Transformer box) and requires stor- ing a full model copy for each task. We propose prefix-tuning (bottom), which freezes the Transformer parameters and only optimizes the prefix (the red pre- fix blocks). Consequently, we only need to store the prefix for each task, making prefix-tuning modular and space-efficient. Note that each vertical block denote transformer activations at one time step. (Rebuffi et al., 2017; Houlsby et al., 2019) inserts additional task-specific layers between the layers of pretrained language models. Adapter-tuning has promising performance on natural language understanding and generation benchmarks, attain- ing comparable performance with fine-tuning while adding only around 2-4% task-specific parameters (Houlsby et al., 2019; Lin et al., 2020). On the extreme end, GPT-3 (Brown et al., 2020) can be deployed without any task-specific tuning. Instead, users prepend a natural language task in- struction (e.g., TL;DR for summarization) and a few examples to the task input; then generate the output from the LM. This approach is known as in-context learning or prompting. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural lan- guage generation (NLG) tasks, inspired by prompt- ing. Consider the task of generating a textual de- Fine-tuning Transformer (Translation) Transformer (Summarization) Transformer (Table-to-text) name Starbucks type coffee shop [SEP] Starbucks serves coffee Input (table-to-text) Prefix (Translation) Output (table-to-text) Prefix-tuning Prefix (Summarization) Prefix (Table-to-text) Transformer (Pretrained) name Starbucks type coffee shop [SEP] Starbucks serves coffee Input (table-to-text) Output (table-to-text)', 'scription of a data table, as shown in Figure 1, where the task input is a linearized table (e.g., “name: Starbucks | type: coffee shop”) and the out- put is a textual description (e.g., “Starbucks serves coffee.”). Prefix-tuning prepends a sequence of continuous task-specific vectors to the input, which we call a prefix, depicted by red blocks in Figure 1 (bottom). For subsequent tokens, the Transformer can attend to the prefix as if it were a sequence of “virtual tokens”, but unlike prompting, the prefix consists entirely of free parameters which do not correspond to real tokens. In contrast to fine-tuning in Figure 1 (top), which updates all Transformer parameters and thus requires storing a tuned copy of the model for each task, prefix-tuning only op- timizes the prefix. Consequently, we only need to store one copy of the large Transformer and a learned task-specific prefix, yielding a very small overhead for each additional task (e.g., 250K pa- rameters for table-to-text). In contrast to fine-tuning, prefix-tuning is mod- ular: we train an upstream prefix which steers a downstream LM, which remains unmodified. Thus, a single LM can support many tasks at once. In the context of personalization where the tasks cor- respond to different users (Shokri and Shmatikov, 2015; McMahan et al., 2016), we could have a sep- arate prefix for each user trained only on that user’s data, thereby avoiding data cross-contamination. Moreover, the prefix-based architecture enables us to even process examples from multiple users/tasks in a single batch, something that is not possible with other lightweight fine-tuning approaches. We evaluate prefix-tuning on table-to-text gen- eration using GPT-2 and abstractive summariza- tion using BART. In terms of storage, prefix-tuning stores 1000x fewer parameters than fine-tuning. In terms of performance when trained on full datasets, prefix-tuning and fine-tuning are comparable for table-to-text (§6.1), while prefix-tuning suffers a small degradation for summarization (§6.2). In low- data settings, prefix-tuning on average outperforms fine-tuning on both tasks (§6.3). Prefix-tuning also extrapolates better to tables (for table-to-text) and articles (for summarization) with unseen topics (§6.4). 2 Related Work Fine-tuning for natural language generation. Current state-of-the-art systems for natural lan- guage generation are based on fine-tuning pre- trained LMs. For table-to-text generation, Kale (2020) fine-tunes a sequence-to-sequence model (T5; Raffel et al., 2020). For extractive and abstrac- tive summarization, researchers fine-tune masked language models (e.g., BERT; Devlin et al., 2019) and encode-decoder models (e.g., BART; Lewis et al., 2020) respectively (Zhong et al., 2020; Liu and Lapata, 2019; Raffel et al., 2020). For other conditional NLG tasks such as machine transla- tion and dialogue generation, fine-tuning is also the prevalent paradigm (Zhang et al., 2020c; Stickland et al., 2020; Zhu et al., 2020; Liu et al., 2020). In this paper, we focus on table-to-text using GPT-2 and summarization using BART, but prefix-tuning can be applied to other generation tasks and pre- trained models. Lightweight fine-tuning. Lightweight fine- tuning freezes most of the pretrained parameters and modifies the pretrained model with small trainable modules. The key challenge is to identify high-performing architectures of the modules and the subset of pretrained parameters to tune. One line of research considers removing parameters: some model weights are ablated away by training a binary mask over model parameters (Zhao et al., 2020; Radiya-Dixit and Wang, 2020). Another line of research considers inserting parameters. For example, Zhang et al. (2020a) trains a “side” network that is fused with the pretrained model via summation; adapter-tuning inserts task-specific lay- ers (adapters) between each layer of the pretrained LM (Houlsby et al., 2019; Lin et al., 2020; Rebuffi et al., 2017; Pfeiffer et al., 2020). Compared to this line of work, which tunes around 3.6% of the LM parameters, our method obtains a further 30x reduction in task-specific parameters, tuning only 0.1% while maintaining comparable performance. Prompting. Prompting means prepending in- structions and a few examples to the task input and generating the output from the LM. GPT-3 (Brown et al., 2020) uses manually designed prompts to adapt its generation for different tasks, and this framework is termed in-context learning. However, since Transformers can only condition on a bounded-length context (e.g., 2048 tokens for GPT- 3), in-context learning is unable to fully exploit training sets longer than the context window. Sun and Lai (2020) also prompt by keywords to control for sentiment or topic of the generated sentence. In natural language understanding tasks, prompt', 'significantly improves generation, as shown in Fig- ure 5. In particular, initializing with task relevant words such as “summarization” and “table-to-text” obtains slightly better performance than task irrelevant words such as “elephant” and “divide”, but using real words is still better than random. Since we initialize the prefix with activations of real words computed by the LM, this initial- ization strategy is concordant with preserving the pretrained LM as much as possible. 8 Discussion In this section, we will discuss several favorable properties of prefix-tuning and some open prob- lems. 8.1 Personalization As we note in §1, prefix-tuning is advantageous when there are a large number of tasks that needs to be trained independently. One practical setting is user privacy (Shokri and Shmatikov, 2015; McMa- han et al., 2016). In order to preserve user privacy, each user’s data needs to be separated and a per- sonalized model needs to be trained independently for each user. Consequently, each user can be re- garded as an independent task. If there are millions of users, prefix-tuning can scale to this setting and maintain modularity, enabling flexible addition or deletion of users by adding or deleting their pre- fixes without cross-contamination. 8.2 Batching Across Users Under the same personalization setting, prefix- tuning allows batching different users’ queries even though they are backed by different prefixes. When multiple users query a cloud GPU device with their inputs, it is computationally efficient to put these users in the same batch. Prefix-tuning keeps the shared LM intact; consequently, batching requires a simple step of prepending the personalized prefix to user input, and all the remaining computation is unchanged. In contrast, we can’t batch across different users in adapter-tuning, which has person- alized adapters between shared Transformer layers. 8.3 Inductive Bias of Prefix-tuning Recall that fine-tuning updates all pretrained pa- rameters, whereas prefix-tuning and adapter-tuning preserve them. Since the language models are pre- trained on general purpose corpus, preserving the LM parameters might help generalization to do- mains unseen during training. In concordance with this intuition, we observe that both prefix-tuning and adapter-tuning have significant performance gain in extrapolation settings (§6.4); however, the reason for such gain is an open question. While prefix-tuning and adapter-tuning both freeze the pretrained parameters, they tune different sets of parameters to affect the activation layers of the Transformer. Recall that prefix-tuning keeps the LM intact and uses the prefix and the pretrained at- tention blocks to affect the subsequent activations; adapter-tuning inserts trainable modules between LM layers, which directly add residual vectors to the activations. Moreover, we observe that prefix- tuning requires vastly fewer parameters compared to adapter-tuning while maintaining comparable performance. We think this gain in parameter effi- ciency is because prefix-tuning keeps the pretrained LM intact as much as possible, and therefore ex- ploits the LM more than adapter-tuning. Concurrent work by Aghajanyan et al. (2020) uses intrinsic dimension to show that there exists a low dimension reparameterization that is as ef- fective for fine-tuning as the full parameter space. This explains why good accuracy on downstream task can be obtained by updating only a small num- ber of parameters. Our work echoes the finding by showing that good generation performance can be attained by updating a very small prefix. 9 Conclusion We have proposed prefix-tuning, a lightweight al- ternative to fine-tuning that prepends a trainable continuous prefix for NLG tasks. We discover that despite learning 1000x fewer parameters than fine- tuning, prefix-tuning can maintain a comparable performance in a full data setting and outperforms fine-tuning in both low-data and extrapolation set- tings. References Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Anja Belz and Ehud Reiter. 2006. Comparing auto- matic and human evaluation of NLG systems. In 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda']```\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
    "Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
    "\n",
    "User Query: ```{query}```\n",
    "List of Extracted Pages: ```{top_results['page_chunks'].to_list()}```\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix-tuning is a lightweight alternative to fine-tuning for natural language generation tasks. It keeps language model parameters frozen but optimizes a small continuous task-specific vector called the prefix. Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". It is modular and can support many tasks at once. Prefix-tuning stores 1000x fewer parameters than fine-tuning and is advantageous when there are a large number of tasks that need to be trained independently, such as in user privacy settings. It allows batching different users' queries even though they are backed by different prefixes, making it computationally efficient. Prefix-tuning requires vastly fewer parameters compared to adapter-tuning while maintaining comparable performance.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt, chat_model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_search(query, count=10):\n",
    "    results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "    page_chunks = []\n",
    "    for result in results:\n",
    "        page_chunks.append(result['page_text'])\n",
    "        \n",
    "    #Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "    embed_df['embedding'] = embed_df[\"page_chunks\"].apply(lambda page_text : get_embedding(page_text, engine = embedding_model))\n",
    "\n",
    "    query_embedding = get_embedding(query, engine=embedding_model)\n",
    "    embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "    top_results = (\n",
    "        embed_df.sort_values(\"similarities\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "        .head(3)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
    "    Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
    "\n",
    "    User Query: ```{query}```\n",
    "    List of Extracted Pages: ```{top_results['page_chunks'].to_list()}```\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_completion(prompt, chat_model)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix-tuning is a lightweight alternative to fine-tuning for natural language generation tasks. It keeps the language model parameters frozen and optimizes a small continuous task-specific vector called the prefix. This prefix can be thought of as a sequence of \"virtual tokens\" that subsequent tokens can attend to. Prefix-tuning draws inspiration from prompting and can be applied to various generation tasks and pre-trained models. It requires storing only one copy of the large Transformer and a learned task-specific prefix, yielding a very small overhead for each additional task. Prefix-tuning is modular and can support many tasks at once. It is advantageous when there are a large number of tasks that need to be trained independently, such as in user privacy settings. Prefix-tuning allows batching different users' queries even though they are backed by different prefixes, making it computationally efficient. Prefix-tuning and adapter-tuning both freeze the pre-trained parameters, but they tune different sets of parameters to affect the activation layers of the Transformer. Prefix-tuning requires vastly fewer parameters compared to adapter-tuning while maintaining comparable performance.\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"How does prompt tuning work?\", 5)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fully zero-shot dense retrieval system is a system that can retrieve relevant information without any training on the specific task or domain. The extracted pages provide information on various methods and techniques used for natural language processing tasks such as summarization and table-to-text generation, as well as performance metrics such as ROUGE, NIST, METEOR, and CIDEr. The pages also discuss the effectiveness of prefix-tuning compared to fine-tuning and adapter-tuning in low-data regimes, as well as the extrapolation performance of language models to unseen categories or topics.\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"what is a fully zero-shot dense retrieval system?\", 10)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
